services:
  postgres:
    image: postgres:16-alpine
    container_name: xx-nest-postgres
    environment:
      POSTGRES_USER: xx_nest_user
      POSTGRES_PASSWORD: xx_nest_password
      POSTGRES_DB: xx_nest_db
    ports:
      - '5432:5432'
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U xx_nest_user -d xx_nest_db']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: xx-nest-redis
    ports:
      - '6666:6379'
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  whisper-api:
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: xx-nest-whisper
    environment:
      - WHISPER_MODEL=large-v3
      - WHISPER_BACKEND=faster-whisper
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - PRELOAD_MODELS=["large-v3"]    
      - MODEL_TTL=-1
    ports:
      - '8000:8000'
    volumes:
      - ./whisper-models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    # 非 GPU 環境下移除上方 deploy 區塊並使用此版本
    # image: fedirz/faster-whisper-server:latest-cpu

  ollama:
    image: ollama/ollama:latest
    container_name: xx-nest-ollama
    ports:
      - '11434:11434'
    volumes:
      - ./ollama-data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # 自動下載模型的輔助容器
  ollama-pull-model:
    image: ollama/ollama:latest
    container_name: xx-nest-ollama-setup
    volumes:
      - ./ollama-data:/root/.ollama
    entrypoint: /bin/sh -c "ollama serve & sleep 5 && ollama pull thirdeyeai/DeepSeek-R1-Distill-Qwen-7B-uncensored:Q4_0"
    depends_on:
      - ollama

  # Ollama 圖形介面
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: xx-nest-webui
    ports:
      - '3000:8080'
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WHISPER_MODEL=large-v3'
      - 'OPENAI_API_BASE_URL=http://whisper-api:8000/v1'
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  postgres-data:
  redis-data:
  open-webui-data: