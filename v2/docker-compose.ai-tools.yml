# docker-compose -f docker-compose.ai-tools.yml up -d
# 請部屬在 wsl2 環境下
services:
  whisper-api:
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: xx-nest-whisper
    environment:
      - WHISPER_MODEL=large-v3
      - WHISPER_BACKEND=faster-whisper
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - PRELOAD_MODELS=["large-v3"]    
      - MODEL_TTL=-1
    ports:
      - '8000:8000'
    volumes:
      - ./whisper-models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    # 非 GPU 環境下移除上方 deploy 區塊並使用此版本
    # image: fedirz/faster-whisper-server:latest-cpu

  ollama:
    image: ollama/ollama:latest
    container_name: xx-nest-ollama
    ports:
      - '11434:11434'
    volumes:
      - ./ollama-data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # 自動下載模型的輔助容器
  ollama-pull-model:
    image: ollama/ollama:latest
    container_name: xx-nest-ollama-setup
    volumes:
      - ./ollama-data:/root/.ollama
    entrypoint: /bin/sh -c "ollama serve & sleep 5 && ollama pull thirdeyeai/DeepSeek-R1-Distill-Qwen-7B-uncensored:Q4_0"
    depends_on:
      - ollama

  # Ollama 圖形介面
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: xx-nest-webui
    ports:
      - '2951:8080'
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WHISPER_MODEL=large-v3'
      - 'OPENAI_API_BASE_URL=http://whisper-api:8000/v1'
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  open-webui-data:
